{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv8 model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "import os\n",
    "\n",
    "os.environ[\"DATASET_DIRECTORY\"] = \"C:/content/datasets\"\n",
    "\n",
    "rf = Roboflow(api_key=\"feG2z6aSS9JkkauzQ8Qo\")\n",
    "project = rf.workspace(\"lidar-object-detection\").project(\"lidar-human-detection\")\n",
    "dataset = project.version(2).download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ouster import client\n",
    "from ouster import pcap\n",
    "from ultralytics import YOLO\n",
    "from ouster.client import Scans, XYZLut, SensorInfo, destagger\n",
    "from contextlib import closing\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def avg_time_of_detection(weight_path):\n",
    "    detection_time = []\n",
    "    model = YOLO(weight_path)\n",
    "\n",
    "    metadata_path = \"C:/Users/szyme/Ouster/data/PKR_test1/test4.json\"\n",
    "    pcap_path = \"C:/Users/szyme/Ouster/data/PKR_test1/test4.pcap\"\n",
    "\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = client.SensorInfo(f.read())\n",
    "\n",
    "    pcap_file = pcap.Pcap(pcap_path, metadata)\n",
    "    xyz_lut = client.XYZLut(metadata)\n",
    "\n",
    "    with closing(Scans(pcap_file)) as scans:\n",
    "        i = 0\n",
    "        for scan in scans:\n",
    "            i += 1\n",
    "            sig_field = scan.field(client.ChanField.SIGNAL)\n",
    "            sig_destaggered = destagger(metadata, sig_field)\n",
    "            scaling_factor = 0.004\n",
    "            scaled_arr = sig_destaggered / (0.5 + scaling_factor * sig_destaggered)\n",
    "            signal_image = scaled_arr.astype(np.uint8)\n",
    "            combined_img = np.dstack((signal_image, signal_image, signal_image))\n",
    "\n",
    "            results = model.predict(source=combined_img, imgsz=1024, tracker='bytetrack.yaml', verbose=False)\n",
    "\n",
    "            speed = results[0].speed\n",
    "            time = speed['preprocess'] + speed['inference'] + speed['postprocess']\n",
    "            detection_time.append(time)\n",
    "            \n",
    "            if i >= 100:   \n",
    "                break\n",
    "    return sum(detection_time)/len(detection_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.05890250205994\n"
     ]
    }
   ],
   "source": [
    "avg_time = avg_time_of_detection('../weights/best_3000_s.pt')\n",
    "print(avg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.429986000061035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.208  Python-3.9.13 torch-2.1.0+cpu CPU (AMD Ryzen 7 6800HS with Radeon Graphics)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\content\\datasets\\lidar-human-detection-2\\valid\\labels.cache... 269 images, 47 backgrounds, 0 corrupt: 100%|██████████| 269/269 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:06<00:00,  2.48it/s]\n",
      "                   all        269        662      0.885      0.866      0.924       0.55\n",
      "Speed: 0.3ms preprocess, 21.1ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val6\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.09240245819092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.208  Python-3.9.13 torch-2.1.0+cpu CPU (AMD Ryzen 7 6800HS with Radeon Graphics)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\content\\datasets\\lidar-human-detection-2\\valid\\labels.cache... 269 images, 47 backgrounds, 0 corrupt: 100%|██████████| 269/269 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:06<00:00,  2.49it/s]\n",
      "                   all        269        662       0.87        0.9      0.934      0.571\n",
      "Speed: 0.3ms preprocess, 21.1ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val7\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.93436908721924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.208  Python-3.9.13 torch-2.1.0+cpu CPU (AMD Ryzen 7 6800HS with Radeon Graphics)\n",
      "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\content\\datasets\\lidar-human-detection-2\\valid\\labels.cache... 269 images, 47 backgrounds, 0 corrupt: 100%|██████████| 269/269 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:19<00:00,  1.12s/it]\n",
      "                   all        269        662      0.891      0.869      0.933      0.582\n",
      "Speed: 0.3ms preprocess, 64.9ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val8\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.524595737457275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.208  Python-3.9.13 torch-2.1.0+cpu CPU (AMD Ryzen 7 6800HS with Radeon Graphics)\n",
      "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\content\\datasets\\lidar-human-detection-2\\valid\\labels.cache... 269 images, 47 backgrounds, 0 corrupt: 100%|██████████| 269/269 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:15<00:00,  1.09it/s]\n",
      "                   all        269        662       0.86      0.902      0.933      0.575\n",
      "Speed: 0.3ms preprocess, 53.2ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val9\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "\n",
    "directory = '../weights/'\n",
    "\n",
    "detection_results = {}\n",
    " \n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    if os.path.isfile(f):\n",
    "        # print(f)\n",
    "        avg_time = avg_time_of_detection(str(f))\n",
    "        print(avg_time)\n",
    "        model = YOLO(f)\n",
    "        metrics = model.val()\n",
    "        formatted_results = {}\n",
    "        for key, val in metrics.results_dict.items():\n",
    "            if key != 'fitness':\n",
    "                formatted_results[key[8:-3]] = val\n",
    "            else:\n",
    "                formatted_results[key] = val\n",
    "        detection_results[f] = [avg_time, formatted_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../weights/best_3000_n.pt :  32.429986000061035 ms, {'precision': 0.8852701575502824, 'recall': 0.8655589123867069, 'mAP50': 0.9244299681155909, 'mAP50-95': 0.5504321634699545, 'fitness': 0.5878319439345181}\n",
      "../weights/best_3000_n_200.pt :  34.09240245819092 ms, {'precision': 0.8700962954286553, 'recall': 0.9004879694639911, 'mAP50': 0.9344139833233811, 'mAP50-95': 0.5709837026104664, 'fitness': 0.6073267306817579}\n",
      "../weights/best_3000_s.pt :  63.93436908721924 ms, {'precision': 0.8909035734398139, 'recall': 0.8685800604229608, 'mAP50': 0.9328926085129733, 'mAP50-95': 0.5824135092485827, 'fitness': 0.6174614191750217}\n",
      "../weights/best_3000_s_200.pt :  59.524595737457275 ms, {'precision': 0.8602582485491604, 'recall': 0.9020207590668169, 'mAP50': 0.9334241611932032, 'mAP50-95': 0.575082108600139, 'fitness': 0.6109163138594453}\n"
     ]
    }
   ],
   "source": [
    "for key, val in detection_results.items():\n",
    "    print(f\"{key} :  {val[0]} ms, {val[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
