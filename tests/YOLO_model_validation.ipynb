{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv8 model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dependency ultralytics==8.0.196 is required but found version=8.0.208, to fix: `pip install ultralytics==8.0.196`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in C:/content/datasets/lidar-human-detection-2 to yolov8:: 100%|██████████| 92241/92241 [00:08<00:00, 10676.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to C:/content/datasets/lidar-human-detection-2 in yolov8:: 100%|██████████| 6458/6458 [00:02<00:00, 2259.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "import os\n",
    "\n",
    "os.environ[\"DATASET_DIRECTORY\"] = \"C:/content/datasets\"\n",
    "\n",
    "rf = Roboflow(api_key=\"feG2z6aSS9JkkauzQ8Qo\")\n",
    "project = rf.workspace(\"lidar-object-detection\").project(\"lidar-human-detection\")\n",
    "dataset = project.version(2).download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ouster import client\n",
    "from ouster import pcap\n",
    "from ultralytics import YOLO\n",
    "from ouster.client import Scans, XYZLut, SensorInfo, destagger\n",
    "from contextlib import closing\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def avg_time_of_detection(weight_path):\n",
    "    detection_time = []\n",
    "    model = YOLO(weight_path)\n",
    "\n",
    "    metadata_path = \"C:/Users/szyme/Ouster/data/PKR_test1/test4.json\"\n",
    "    pcap_path = \"C:/Users/szyme/Ouster/data/PKR_test1/test4.pcap\"\n",
    "\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = client.SensorInfo(f.read())\n",
    "\n",
    "    pcap_file = pcap.Pcap(pcap_path, metadata)\n",
    "    xyz_lut = client.XYZLut(metadata)\n",
    "\n",
    "    with closing(Scans(pcap_file)) as scans:\n",
    "        i = 0\n",
    "        for scan in scans:\n",
    "            i += 1\n",
    "            sig_field = scan.field(client.ChanField.SIGNAL)\n",
    "            sig_destaggered = destagger(metadata, sig_field)\n",
    "            scaling_factor = 0.004\n",
    "            scaled_arr = sig_destaggered / (0.5 + scaling_factor * sig_destaggered)\n",
    "            signal_image = scaled_arr.astype(np.uint8)\n",
    "            combined_img = np.dstack((signal_image, signal_image, signal_image))\n",
    "\n",
    "            results = model.predict(source=combined_img, imgsz=1024, tracker='bytetrack.yaml', verbose=False)\n",
    "\n",
    "            speed = results[0].speed\n",
    "            time = speed['preprocess'] + speed['inference'] + speed['postprocess']\n",
    "            detection_time.append(time)\n",
    "            \n",
    "            if i >= 100:   \n",
    "                break\n",
    "    return sum(detection_time)/len(detection_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118.77604722976685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.208  Python-3.9.13 torch-2.1.0+cpu CPU (AMD Ryzen 7 6800HS with Radeon Graphics)\n",
      "Model summary (fused): 218 layers, 25840339 parameters, 0 gradients, 78.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\content\\datasets\\lidar-human-detection-1\\valid\\labels.cache... 269 images, 47 backgrounds, 0 corrupt: 100%|██████████| 269/269 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:32<00:00,  1.92s/it]\n",
      "                   all        269        662      0.869      0.894      0.931      0.578\n",
      "Speed: 0.3ms preprocess, 116.8ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.40929794311523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.208  Python-3.9.13 torch-2.1.0+cpu CPU (AMD Ryzen 7 6800HS with Radeon Graphics)\n",
      "Model summary (fused): 218 layers, 25840339 parameters, 0 gradients, 78.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\content\\datasets\\lidar-human-detection-2\\valid\\labels.cache... 269 images, 47 backgrounds, 0 corrupt: 100%|██████████| 269/269 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:35<00:00,  2.11s/it]\n",
      "                   all        269        662      0.884      0.883      0.935      0.583\n",
      "Speed: 0.3ms preprocess, 128.7ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.35186052322388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.208  Python-3.9.13 torch-2.1.0+cpu CPU (AMD Ryzen 7 6800HS with Radeon Graphics)\n",
      "Model summary (fused): 218 layers, 25840339 parameters, 0 gradients, 78.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\content\\datasets\\lidar-human-detection-2\\valid\\labels.cache... 269 images, 47 backgrounds, 0 corrupt: 100%|██████████| 269/269 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:33<00:00,  1.96s/it]\n",
      "                   all        269        662      0.912      0.858      0.931      0.576\n",
      "Speed: 0.3ms preprocess, 119.0ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.66836929321289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.208  Python-3.9.13 torch-2.1.0+cpu CPU (AMD Ryzen 7 6800HS with Radeon Graphics)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\content\\datasets\\lidar-human-detection-2\\valid\\labels.cache... 269 images, 47 backgrounds, 0 corrupt: 100%|██████████| 269/269 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.32it/s]\n",
      "                   all        269        662      0.885      0.866      0.924       0.55\n",
      "Speed: 0.3ms preprocess, 22.7ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.98355484008789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.208  Python-3.9.13 torch-2.1.0+cpu CPU (AMD Ryzen 7 6800HS with Radeon Graphics)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\content\\datasets\\lidar-human-detection-2\\valid\\labels.cache... 269 images, 47 backgrounds, 0 corrupt: 100%|██████████| 269/269 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.31it/s]\n",
      "                   all        269        662       0.87        0.9      0.934      0.571\n",
      "Speed: 0.3ms preprocess, 22.8ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val5\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.51349115371704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.208  Python-3.9.13 torch-2.1.0+cpu CPU (AMD Ryzen 7 6800HS with Radeon Graphics)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\content\\datasets\\lidar-human-detection-2\\valid\\labels.cache... 269 images, 47 backgrounds, 0 corrupt: 100%|██████████| 269/269 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]\n",
      "                   all        269        662      0.863      0.837        0.9      0.537\n",
      "Speed: 0.3ms preprocess, 22.5ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val6\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.80268573760986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.208  Python-3.9.13 torch-2.1.0+cpu CPU (AMD Ryzen 7 6800HS with Radeon Graphics)\n",
      "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\content\\datasets\\lidar-human-detection-2\\valid\\labels.cache... 269 images, 47 backgrounds, 0 corrupt: 100%|██████████| 269/269 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:16<00:00,  1.06it/s]\n",
      "                   all        269        662      0.891      0.869      0.933      0.582\n",
      "Speed: 0.3ms preprocess, 55.1ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val7\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.065436363220215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.208  Python-3.9.13 torch-2.1.0+cpu CPU (AMD Ryzen 7 6800HS with Radeon Graphics)\n",
      "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\content\\datasets\\lidar-human-detection-2\\valid\\labels.cache... 269 images, 47 backgrounds, 0 corrupt: 100%|██████████| 269/269 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:16<00:00,  1.04it/s]\n",
      "                   all        269        662       0.86      0.902      0.933      0.575\n",
      "Speed: 0.3ms preprocess, 56.1ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val8\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.611849308013916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.208  Python-3.9.13 torch-2.1.0+cpu CPU (AMD Ryzen 7 6800HS with Radeon Graphics)\n",
      "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\content\\datasets\\lidar-human-detection-2\\valid\\labels.cache... 269 images, 47 backgrounds, 0 corrupt: 100%|██████████| 269/269 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:16<00:00,  1.05it/s]\n",
      "                   all        269        662      0.857      0.893      0.921       0.56\n",
      "Speed: 0.3ms preprocess, 55.8ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val9\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "\n",
    "directory = '../weights/'\n",
    "\n",
    "detection_results = {}\n",
    " \n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    if os.path.isfile(f):\n",
    "        # print(f)\n",
    "        avg_time = avg_time_of_detection(str(f))\n",
    "        print(avg_time)\n",
    "        model = YOLO(f)\n",
    "        metrics = model.val()\n",
    "        formatted_results = {}\n",
    "        for key, val in metrics.results_dict.items():\n",
    "            if key != 'fitness':\n",
    "                formatted_results[key[8:-3]] = val\n",
    "            else:\n",
    "                formatted_results[key] = val\n",
    "        formatted_results['mAP75'] = metrics.box.map75\n",
    "        detection_results[f] = [avg_time, formatted_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../weights/best_1000_m_200.pt :  118.77604722976685 ms, {'precision': 0.8692426144063043, 'recall': 0.893731455230514, 'mAP50': 0.9306214431611054, 'mAP50-95': 0.5784182247261054, 'fitness': 0.6136385465696055, 'mAP75': 0.6483697820971608}\n",
      "../weights/best_3000_m_100.pt :  120.40929794311523 ms, {'precision': 0.8836347904546593, 'recall': 0.8832479082812953, 'mAP50': 0.9346218530879644, 'mAP50-95': 0.5825829980135697, 'fitness': 0.6177868835210092, 'mAP75': 0.6514991905842425}\n",
      "../weights/best_3000_m_50.pt :  120.35186052322388 ms, {'precision': 0.9123084342595152, 'recall': 0.8580060422960725, 'mAP50': 0.930946122667509, 'mAP50-95': 0.5761372359219116, 'fitness': 0.6116181245964712, 'mAP75': 0.646474652402208}\n",
      "../weights/best_3000_n_100.pt :  34.66836929321289 ms, {'precision': 0.8852701575502824, 'recall': 0.8655589123867069, 'mAP50': 0.9244299681155909, 'mAP50-95': 0.5504321634699545, 'fitness': 0.5878319439345181, 'mAP75': 0.5946786425634137}\n",
      "../weights/best_3000_n_200.pt :  33.98355484008789 ms, {'precision': 0.8700962954286553, 'recall': 0.9004879694639911, 'mAP50': 0.9344139833233811, 'mAP50-95': 0.5709837026104664, 'fitness': 0.6073267306817579, 'mAP75': 0.6286553837507424}\n",
      "../weights/best_3000_n_50.pt :  33.51349115371704 ms, {'precision': 0.8630835620893882, 'recall': 0.8368580060422961, 'mAP50': 0.9000885145480257, 'mAP50-95': 0.5367934609057043, 'fitness': 0.5731229662699365, 'mAP75': 0.5799791823127216}\n",
      "../weights/best_3000_s_100.pt :  58.80268573760986 ms, {'precision': 0.8909035734398139, 'recall': 0.8685800604229608, 'mAP50': 0.9328926085129733, 'mAP50-95': 0.5824135092485827, 'fitness': 0.6174614191750217, 'mAP75': 0.6708197216759347}\n",
      "../weights/best_3000_s_200.pt :  60.065436363220215 ms, {'precision': 0.8602582485491604, 'recall': 0.9020207590668169, 'mAP50': 0.9334241611932032, 'mAP50-95': 0.575082108600139, 'fitness': 0.6109163138594453, 'mAP75': 0.6360646462868097}\n",
      "../weights/best_3000_s_50.pt :  62.611849308013916 ms, {'precision': 0.8566087067845347, 'recall': 0.8933819357898969, 'mAP50': 0.9211935389395547, 'mAP50-95': 0.5602821456007282, 'fitness': 0.5963732849346108, 'mAP75': 0.6265497360858352}\n"
     ]
    }
   ],
   "source": [
    "for key, val in detection_results.items():\n",
    "    print(f\"{key} :  {val[0]} ms, {val[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setup complete  (16 CPUs, 39.2 GB RAM, 398.6/1862.2 GB disk)\n",
      "\n",
      "Benchmarks complete for best_3000_s_100.pt on /content/datasets/lidar-human-detection-2/data.yaml at imgsz=1024 (451.22s)\n",
      "                   Format Status  Size (MB)  metrics/mAP50-95(B)  Inference time (ms/im)\n",
      "0                 PyTorch              21.4               0.5824                   63.16\n",
      "1             TorchScript              43.0               0.5823                  342.03\n",
      "2                    ONNX               0.0                  NaN                     NaN\n",
      "3                OpenVINO               0.0                  NaN                     NaN\n",
      "4                TensorRT               0.0                  NaN                     NaN\n",
      "5                  CoreML               0.0                  NaN                     NaN\n",
      "6   TensorFlow SavedModel               0.0                  NaN                     NaN\n",
      "7     TensorFlow GraphDef               0.0                  NaN                     NaN\n",
      "8         TensorFlow Lite               0.0                  NaN                     NaN\n",
      "9     TensorFlow Edge TPU               0.0                  NaN                     NaN\n",
      "10          TensorFlow.js               0.0                  NaN                     NaN\n",
      "11           PaddlePaddle               0.0                  NaN                     NaN\n",
      "12                   ncnn               0.0                  NaN                     NaN\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Format</th>\n",
       "      <th>Status❔</th>\n",
       "      <th>Size (MB)</th>\n",
       "      <th>metrics/mAP50-95(B)</th>\n",
       "      <th>Inference time (ms/im)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PyTorch</td>\n",
       "      <td>✅</td>\n",
       "      <td>21.4</td>\n",
       "      <td>0.5824</td>\n",
       "      <td>63.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TorchScript</td>\n",
       "      <td>✅</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.5823</td>\n",
       "      <td>342.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ONNX</td>\n",
       "      <td>❌</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OpenVINO</td>\n",
       "      <td>❌</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TensorRT</td>\n",
       "      <td>❌</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CoreML</td>\n",
       "      <td>❌</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TensorFlow SavedModel</td>\n",
       "      <td>❌</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TensorFlow GraphDef</td>\n",
       "      <td>❌</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TensorFlow Lite</td>\n",
       "      <td>❌</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TensorFlow Edge TPU</td>\n",
       "      <td>❌</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TensorFlow.js</td>\n",
       "      <td>❌</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PaddlePaddle</td>\n",
       "      <td>❌</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ncnn</td>\n",
       "      <td>❌</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Format Status❔  Size (MB)  metrics/mAP50-95(B)  Inference time (ms/im)\n",
       "0                 PyTorch       ✅       21.4               0.5824                   63.16\n",
       "1             TorchScript       ✅       43.0               0.5823                  342.03\n",
       "2                    ONNX       ❌        0.0                  NaN                     NaN\n",
       "3                OpenVINO       ❌        0.0                  NaN                     NaN\n",
       "4                TensorRT       ❌        0.0                  NaN                     NaN\n",
       "5                  CoreML       ❌        0.0                  NaN                     NaN\n",
       "6   TensorFlow SavedModel       ❌        0.0                  NaN                     NaN\n",
       "7     TensorFlow GraphDef       ❌        0.0                  NaN                     NaN\n",
       "8         TensorFlow Lite       ❌        0.0                  NaN                     NaN\n",
       "9     TensorFlow Edge TPU       ❌        0.0                  NaN                     NaN\n",
       "10          TensorFlow.js       ❌        0.0                  NaN                     NaN\n",
       "11           PaddlePaddle       ❌        0.0                  NaN                     NaN\n",
       "12                   ncnn       ❌        0.0                  NaN                     NaN"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics.utils.benchmarks import benchmark\n",
    "\n",
    "benchmark(model='../weights/best_3000_s_100.pt', data='/content/datasets/lidar-human-detection-2/data.yaml', imgsz=1024, half=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
